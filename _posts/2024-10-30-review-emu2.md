---
layout: post
title: Emu2
categories: Review
image: /images/2024-10-30-review-emu2-01.png
tags: LMM Diffusion
---

**Year** 2024/05<br>
**arXiv** [https://arxiv.org/abs/2312.132868](https://arxiv.org/abs/2312.13286)<br>
**CVPR 2024**

<hr>

## TL;DR
- New Large Multimodal Model(LMM): Emu2 model.
- Demonstrate strong in-context abilities of Emu2 model.

<hr>

## Backgrounds
### Large Multimodal Models
Recently, Large Language Model(LLM) has great abilities for understandig and generating sentence but not for other modalities like images.
Large Multimodal Models(LMM) is LLM with abilities to handle multimodal data.
There are some approach to add abilities for multimodal data (visual data)

- **Using single model**: Chameleon, Transfusion, Emu3
- **Using additional model**: Emu, Emu2

![Image](/images/2024-10-30-review-emu2-02.png)

LMM models like *Chameleon* handle image data **without additional encoder or decoder** by tokenizing image like text.
*Transfusion* model also do not use additional visual model by performing autoregressive generation and diffusion with one single LLM.<br>
Besides, models like *Emu2* use **additional visual encoder and decoder** to make LLM understand and generate visual data.

### In-Context learning
If we want to solve new task using existing model, model is fine-tuned using datasets for that task.
But some models like LLM have ability to solve new task using given context without additional parameter tuning.
Like this, in-context learning means that learning from given prompt or context without parameter tuning.
- **Zero-shot setting**: Without given example, model can solve given task.
- **One-shot setting**: With one example for task, model can solve task like given example.
- **Few-shot setting**: Solve task with some examples.

<hr>

## Intro 
### Why in-context learning
Previous multimiodal model largely rely on desing task-specific architecture and needs large scale dataset for training.
When new task encountered, these model needs to be trained.
By contrast, humans can solve a new task in context. With few samples, human can solve new task.
Recently, large language models show their in-context capabilities with scaling-up
and large multimodal model which uses LLM also can use in-context learning with scaling-up
### New LMM: Emu2
Emu2 is Large Multimodal Model with 37-billion-parameter.
With existing LLM model, LLaMA, visual encoder and visual decoder are attached to understand and generate image data.
### In this paper
New Emu2 model is evaluated to demonstrate its in-context abilities.
With few-shot setting, model can access many examples and it demonstrates state-of-the-art few-shot performancee on some tasks.
Also with instruction tuning, the model is tuned to follow specific instructions.

<hr>

## Emu2 Model
### Model Overview

![Image](/images/2024-10-30-review-emu2-03.png)

Emu2 uses existing LLM as its base model. LLM has good performance to generate text, but not for images.
To handle visual data like image, Emu2 uses additional visual encoder and decoder.

Emu2 consists of three components:
- **Visual Encoder**: Convert image to continuous embeddings for LLM input. pretrained EVA-02-CLIP-E-plus model is used.
- **Multimodal Modeling**: LLM for generating sequence. pretrained LLaMA-33B is used.
- **Visual Decoder**: Generate image according LLM outputs using diffusion model. pretrained SDXL is used.

To connect visual encoder and multimodal modeling, mean pooling is used to get 8X8 image patches and than linear projection is needed.
While Emu model uses an additional C-Former, Emu2 uses more simple architecture.

### Visual Encoder
For LLM text input, text sequence is first tokenized using tokenizer and converted into continuous embedding space using embedding layer.
In the same manner, visual data have to be converted into continuous embedding space and visual encoder can do this job.
Emu2 uses EVA-02-CLIP-E-plus model as visual encoder, convert 224x224 image into visual embedding.

How visual encoder convert image into embedding for LLM?

![Image](/images/2024-10-30-review-emu2-04.png){: width="500"}

1. Using *EVA-02-CLIP-E-plus model*, convert an image into 1792 dimmension **32x32 patch embeddings**.
2. Using *mean pooling*, convert 32x32 patch embeddings into **8x8 patch embeddings**.
3. Using *linear projection*, **change dimmension to 6656**, which is corresponding with LLM hidden state dimmension. 

As a result, an image converted into 64 patch embeddings, which is **treated as 64 text token** by LLM.

### Visual Decoder
How LLM determine which text token comes next? <br>
Using hidden state of last transformer layer, classification head(lm_head) outputs probabilities for the next token. <br>
For visual data generation, **visual decoder** is used instead of classification head.<br>
Visual embedding will be hidden state between [IMG] and [/IMG] tokens and visual decoder uses it as **conditioning to generate an image**.<br>
Emu2 uses diffusion model, SDXL, as a visual decoder.

![Image](/images/2024-10-30-review-emu2-05.png){: width="500"}

1. LLM produces visual embedding as **64-token-length hidden states** between [IMG] and [/IMG] token. 
2. Using linear projection, **change dimension to 1792** for visual decoder conditioning input.
3. Using visual embedding as **conditioning**, visual decoder generate an image.

With visual decoder for video generation instead of image, Emu2 also can generate video clips.<br>
Emu2 uses Stable Diffusion 2.1 for video generation by changing 2D U-Net as 3D U-Net with temporal convolution.

### Pretraining (Visual Encoder + LLM)
When training Emu2 model, the visual decoder is trained separately from LLM (not end-to-end training).

Training visual encoder and LLM proceeds in two steps.
1. Training for **text generation**<br>
Using image-text and video-text dataset, LLM is trained to generate text.
In this stage, training proceeds only for text generation, and **make LLM to understand visual embedding** of visual encoder.
2. Training for **visual embedding generation**<br>
Using addition visual-text interleaved dataset, LLM will be trained to **generate visual embedding** which will be used by visual decoder.<br>
In this stage, only linear projection and LLM is trained and visual encoder is frozen.<br>
Both image regression loss and text classification loss are used.

Then what is ground truth of visual embedding?<br>
In my opinion, LLM will be trained using visual embedding generated by visual encoder as ground truth.
And this will be the reason of why visual encoder is frozen in step 2.<br>
As a result, input visual embedding from encoder and output visual embedding for decoder will be in same embedding space.

<details>
<summary> (toggle) Pretraining Details</summary>
<div markdown="1">
<h4> Datasets </h4>
- Image-text pairs: LAION-2B, CapsFusion-120M
- Video-text paris: WebVid-10M
- Interleaved image-text data: Multimodal-C4
- Interleaved video-text data: YT-Storyboard-1B
- Grounded image-text pairs: GRIT-20M, CapsFusion-grounded-100M
- Language-only data: Pile (to retain textual reasoning capability)

<h4> First Training Setup </h4>
- Datasets: image-text pairs (162M), video-text pairs (7M)
- Start with 224x224 resized image and use 448x448 after 4000 iteration
</div>

<h4> Second Training Setup </h4>
- Datasets: All datasets including inter-leaved data
- All images are resized to 448x448
</details>

### Pretraining (Visual Decoder)

![Image](/images/2024-10-30-review-emu2-06.png){: width="500"}

As mentioned, visual decoder uses embeddings in the same space as output of the visual encoder.
So visual decoder can be trained only with visual decoder and **do not need LLM inference**.<br>
With visual encoder, visual decoder is **trained like an autoencoder**. In this case, visual embedding space is treated as latent space of autoencoder.

Only U-Net in visual decoder is trained and visual Encoder and VAE of SDXL are frozen.
While visual encoder takes an 448x448 image, visual decoder **generates an 1024x1024 image**.

<details>
<summary> (toggle) Visual Decoder Pretraining Details</summary>
<div markdown="1">
<h4> Datasets </h4>
Images with lower resolution than 512x512 are filtered out.
- LAION-COCO
- LAION-Aesthetics

<h4> Training Setup </h4>
- Input image: 448x448 resolution
- Output image: 1024x1024 resolution
- Classifier-free guidance: randomly discard visual embeddings with probability of 10%

</div>
</details>

<hr>

## Instruction Tuning
Main topic of this paper is about in-context learning ability of Emu2 model.
But Emu2 model also can be fine-tuned for some specific tasks.
From base Emu2 model, two model are made by instruction tuning.

- Emu2-Chat: Response in dialogue following multimodal questions.
- Emu2-Gen: Generating image with text, locations, images conditioning

### Emu2-Chat

What can Emu2-Chat do?
- Conversational with visual data
- Visual question answering
- Lots more with text & visual data

To training Emu2-Chat, data are organized in following chat-like format:<br>
<abbr>\<Sys.Msg\> [USER]: \<Instruction\> [ASSISTANT]: \<Answer\></abbr><br>
Only loss for answer section is used.<br>

One different things from the base model is that visual encoder stage convert each image to 16x16 tokens.
This is more than 8x8 tokens for base model, while capture more intricate spatial details.<br>
Also, video data can be used by sampling video frames. Number of frames is randomly chosen from 8, 12, and 16.
<details>
<summary>Fine-tuning Details</summary>
<div markdown="1">
<h4> Datasets </h4>
Both academic-task-oriented datasets and multimodal chat data are used.<br>
- Academic-task-oriented data
    - Image captioning
    - Visual question answering
    - Knowledgeable question answering
    - Multimodal classification
    - Referring expression comprehension
- Multimodal chat data
    - GPT-assisted visual instruction
    - Language instruction
    - Clock reading
    - Video chat

<h4> Training Setup </h4>
- Sequence length limit: 2048
- Image, video resolution: 448x448
</div>
</details>

### Emu2-Gen

What can Emu2-Gen do?
- Text-to-image generation
- Image generation from images of object to insert
- Image generation from images and bounding boxes.
- Image editing using text
- Image generation from any sequence

Emu2-Gen generates an image following given mixture of following multimodal data.
- Text
- Object(<abbr>\<p\><\/p\></abbr>)
- Object image(<abbr>[IMG][\IMG]</abbr>)
- Object location with bounding boxes(<abbr>\<coor\>\</coor\></abbr>)

Regression loss is only used for output visual embeddings of last image.
During fine-tuning, Visual Encoder are frozen.

<details>
<summary>Fine tuning detail</summary>
<div markdown="1">
<h4> Dataset </h4>
- High-quality datasets
    - Grounded text-to-image generation: CapsFusion-grounded-100M, GRIT
    - Image editing: InstructPix2Pix
    - Text-to-image task: filtered CapsFusion, LAION-Aesthetics, SA-1B, LAION-High_Resolution
- Some premium sources
    - Unsplash
- Output of Text-to-image systems
    - Midjourney-V5, DALL-E-3

<h4> Training Setup </h4>
- Batch size: 4096
- Training steps: 3k
- Learning rate: linear warms up to 1x10^-5 and decays to zero with cosine schedule. 
- Futher fine-tuning: 900 steps using 500k high-quality pairs with a batch size of 2048.

<h4> Data argumentation </h4>
- Randomly drop entities and object localization imformation: for adaptability and robustness.
- Random background variations: reduce the reliance on image backgrounds.
- Random crop
</div>
</details>

<hr>

## Experiments
### In-context learning (base model)
With base model, zero-shot and few-shot abilities are evaluated on some VQA tasks.<br>

Result
![Image](/images/2024-10-30-review-emu2-07.png){: width="400"}
- Emu2 model shows **powerful in-context ability**
- Improved performance with more samples.
- Outperforms Flamingo-80B and IDEFICS-80B with smaller model (37B)

Here are some few-shot examples.
![Image](/images/2024-10-30-review-emu2-08.png)

### Academic-task-oriented benchmarks (Emu2-Chat)
Emu2-Chat is evaluated on academic-task-oriented benchmarks.<br>

Result

![Image](/images/2024-10-30-review-emu2-09.png)
- Outperforms other models in visual question answering task.
- Also better results on LMM benchmarks.

Here are some multimodal understanding examples.
![Image](/images/2024-10-30-review-emu2-10.png)

### Controllable visual generation (Emu2-Gen)
- Visual encoder and decoder of Emu2 show superior results on autoencoding tasks.
![Image](/images/2024-10-30-review-emu2-11.png){:width="400"}

- Emu2 achieves the state-of-the-art performance among multimodal model in zero-shot text-to-image generation task.
![Image](/images/2024-10-30-review-emu2-12.png){:width="400"}

- Emu2 shows strong zero-shot subject-driven image editing ability on DreamBench.
![Image](/images/2024-10-30-review-emu2-13.png){:width="400"}

Here are some examples of controllable generation.

![Image](/images/2024-10-30-review-emu2-14.png)
<hr>

## Summary
- This paper shows strong in-context learning ability of LMM using new model, Emu2.
- By instruction tuning, Emu2-Chat and Emu2-Gen can follow specific task instructions.
- Emu2-Chat demonstrates SOTA results on VQA and LMM benchmarks.
- Emu2-Gen shows remarkable capability of controllable visual generation.