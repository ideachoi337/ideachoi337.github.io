---
layout: post
title: Generative Multimodal Models are In-Context Learners
categories: Review
image: /images/2024-10-30-review-emu2-01.png
tags: LMM Diffusion
---

**Year** 2024/05<br>
**arXiv** [https://arxiv.org/abs/2312.13286](https://arxiv.org/abs/2312.13286)<br>
**Project page** [https://baaivision.github.io/emu2/](https://baaivision.github.io/emu2/)<br>
**Accepted to CVPR 2024**

![Image](/images/2024-10-30-review-emu2-00.png)

<hr>

## TL;DR
- New **Large Multimodal Model**(LMM): Emu2 model.
- Demonstrate strong **in-context learning** abilities of Emu2 model.

<hr>

## Backgrounds
### Large Multimodal Models
Recently, Large Language Models(LLM) have great abilities in understanding and generating sentence but not for other modalities like images.<br>
With multimodal understanding abilities for LLM, Large Multimodal Models(LMM) can handle data of other modalities.<br>
There are some approach to add multimodal understanding abilities. (text+image)

- **Using single model**: Chameleon, Transfusion, Emu3
- **Using additional model**: Emu, Emu2

![Image](/images/2024-10-30-review-emu2-02.png)

LMM models like *Chameleon* handle image data **without additional encoder or decoder** by tokenizing image and use in the same way as text.
*Transfusion* model also do not use additional visual model by performing autoregressive generation and diffusion with one single LLM.<br>
Besides, models like *Emu2* use **additional visual encoder and decoder** to make LLM understand and generate visual data.

### In-Context learning
If we want to solve new task using existing model, we need to fine-tune model for that task.
But some models like LLM have ability to solve new task only using given query or context without additional parameter tuning.
Like this, in-context learning means learning from given prompt or context without changing model parameter.
- **Zero-shot learning**: Solve given task without example for task.
- **One-shot learning**: Solve given task with one example for task.
- **Few-shot learning**: Solve given task with some examples.

<hr>

## Intro 
### Why in-context learning
Previous multimiodal model largely rely on designing task-specific architecture and needs large scale dataset for training.
When new task encountered, these model needs to be trained.
By contrast, with few samples, humans can solve a new task in context.<br>
Recently, large language models show their in-context capabilities with scaling-up
and large multimodal model which uses LLM also can use in-context learning with scaling-up
### New LMM: Emu2
Emu2 is Large Multimodal Model with 37-billion-parameter.
By attaching visual encoder and visual decoder to LLM model, Emu2 can understand and generate image data.
### In this paper
New Emu2 model is evaluated on some tasks to demonstrate its in-context training abilities.
With few-shot setting, model can access many examples and it demonstrates state-of-the-art few-shot performance on some tasks.
Also there are two additional models by instruction tuning, Emu2-Chat and Emu2-Gen.

<hr>

## Emu2 Model
### Model Overview

![Image](/images/2024-10-30-review-emu2-03.png)

Emu2 uses existing LLM as its base model. LLM has good performance to generate text, but not for images.
To handle visual data like image, Emu2 uses additional visual encoder and decoder.

Emu2 consists of three components:
- **Visual Encoder**: Convert image to continuous embeddings for LLM input. pretrained EVA-02-CLIP-E-plus model is used.
- **Multimodal Modeling**: LLM for generating sequence. pretrained LLaMA-33B is used.
- **Visual Decoder**: Generate image according to LLM outputs using diffusion model. pretrained SDXL is used.

To connect visual encoder and multimodal modeling, mean pooling is used to get 8x8 image patches and linear projection follows.
While Emu model uses an additional C-Former, Emu2 uses more simple architecture.

### Visual Encoder
For LLM text input, text sequence is first tokenized using tokenizer and converted into continuous embedding space using embedding layer.
In the same manner, visual data have to be converted into continuous embedding space and visual encoder can do this job.<br>
Emu2 uses EVA-02-CLIP-E-plus model as visual encoder, convert 448x448 image into visual embedding.

How visual encoder convert image into embedding for LLM?

![Image](/images/2024-10-30-review-emu2-04.png){: width="500"}

1. Using *EVA-02-CLIP-E-plus model*, convert an image into 1792 dimmension **32x32 patch embeddings**.
2. Using *mean pooling*, convert 32x32 patch embeddings into **8x8 patch embeddings**.
3. Using *linear projection*, **change dimmension to 6656**, which is corresponding with LLM hidden state dimmension. 

As a result, an image converted into 64 patch embeddings, which is **treated as 64 text token** by LLM.

### Visual Decoder
How LLM determine which text token comes next? <br>
Using hidden state of last transformer layer, classification head(lm_head) outputs probabilities for the next token. 
For visual data generation, **visual decoder** is used instead of classification head. 
Visual embedding will be hidden state between [IMG] and [/IMG] tokens and visual decoder uses it as **conditioning of diffusion model**.<br>
Emu2 uses diffusion model, SDXL, as a visual decoder.

![Image](/images/2024-10-30-review-emu2-05.png){: width="500"}

1. LLM produces visual embedding as **64-token-length hidden states** between [IMG] and [/IMG] token. 
2. Using linear projection, **change dimension to 1792** for visual decoder conditioning input.
3. Using visual embedding as **conditioning**, visual decoder generate an image.

With visual decoder for video generation instead of image, Emu2 also can generate video clips.
Emu2 uses Stable Diffusion 2.1 for video generation by changing 2D U-Net as 3D U-Net with temporal convolution.

### Pretraining (Visual Encoder + LLM)
When training Emu2 model, the visual decoder is trained separately from LLM (not end-to-end training).

Training visual encoder and LLM proceeds in two steps.
1. Training for **text generation**<br>
Using image-text and video-text dataset, LLM is trained to generate text.
In this stage, training proceeds only for text generation, and **make LLM understand visual embedding** of visual encoder.
2. Training for **visual embedding generation**<br>
Using additional visual-text interleaved dataset, LLM is trained to **generate visual embedding** which will be used by visual decoder.
In this stage, only linear projection and LLM is trained and visual encoder is frozen.<br>
Both image regression loss and text classification loss are used.

Then what is ground truth of visual embedding?<br>
In my opinion, LLM will be trained using visual embedding generated by visual encoder as ground truth.
And this will be the reason of why visual encoder is frozen in step 2.<br>
As a result, input visual embedding from encoder and output visual embedding for decoder will be in same embedding space.

<details>
<summary> (toggle) Pretraining Details</summary>
<div markdown="1">
<h4> Datasets </h4>
- Image-text pairs: LAION-2B, CapsFusion-120M
- Video-text paris: WebVid-10M
- Interleaved image-text data: Multimodal-C4
- Interleaved video-text data: YT-Storyboard-1B
- Grounded image-text pairs: GRIT-20M, CapsFusion-grounded-100M
- Language-only data: Pile (to retain textual reasoning capability)

<h4> First Training Setup </h4>
- Datasets: image-text pairs (162M), video-text pairs (7M)
- Start with 224x224 resized image and use 448x448 after 4000 iteration

<h4> Second Training Setup </h4>
- Datasets: All datasets including interleaved data
- All images are resized to 448x448
</div>
</details>

### Pretraining (Visual Decoder)

![Image](/images/2024-10-30-review-emu2-06.png){: width="500"}

As mentioned, visual decoder uses embeddings in the same space as output of the visual encoder.
So visual decoder can be trained only with visual decoder and **do not need LLM inference**.<br>
With visual encoder, visual decoder is **trained like an autoencoder**. In this case, visual embedding space is treated as latent space of autoencoder.

Only U-Net in visual decoder is trained and visual Encoder and VAE of SDXL are frozen.
While visual encoder takes an 448x448 image, visual decoder **generates an 1024x1024 image**.

<details>
<summary> (toggle) Visual Decoder Pretraining Details</summary>
<div markdown="1">
<h4> Datasets </h4>
Images with lower resolution than 512x512 are filtered out.
- LAION-COCO
- LAION-Aesthetics

<h4> Training Setup </h4>
- Input image: 448x448 resolution
- Output image: 1024x1024 resolution
- Classifier-free guidance: randomly discard visual embeddings with probability of 10%

</div>
</details>

<hr>

## Instruction Tuning
Main topic of this paper is about in-context learning ability of Emu2 model.
But Emu2 model also can be fine-tuned for some specific tasks.
From base Emu2 model, two model are made by instruction tuning.

- **Emu2-Chat**: Response in dialogue following multimodal questions.
- **Emu2-Gen**: Generating image with text, locations, images conditioning

### Emu2-Chat

What can Emu2-Chat do?
- Conversation with visual data
- **Visual question answering**
- Lots more with text and visual data sequences

To training Emu2-Chat, data are organized in following chat-like format:<br>
<abbr>\<Sys.Msg\> [USER]: \<Instruction\> [ASSISTANT]: \<Answer\></abbr><br>
Only loss for answer section is used.<br>

One different things from the base model is that visual encoder stage converts each image to 16x16 tokens.
This is more than 8x8 tokens for base model, capturing more intricate spatial details.<br>
Also, video data can be used by sampling video frames. Number of frames is randomly chosen from 8, 12, and 16.
<details>
<summary>(toggle) Fine-tuning Details</summary>
<div markdown="1">
<h4> Datasets </h4>
Both academic-task-oriented datasets and multimodal chat data are used.<br>
- Academic-task-oriented data
    - Image captioning
    - Visual question answering
    - Knowledgeable question answering
    - Multimodal classification
    - Referring expression comprehension
- Multimodal chat data
    - GPT-assisted visual instruction
    - Language instruction
    - Clock reading
    - Video chat

<h4> Training Setup </h4>
- Sequence length limit: 2048
- Image, video resolution: 448x448
</div>
</details>

### Emu2-Gen

What can Emu2-Gen do?
- Text-to-image generation
- Image generation from images of object to insert
- Image generation from images and bounding boxes
- Image editing using text
- **Image generation from any sequence**

Emu2-Gen generates an image from given mixture of multimodal data.
- Text
- Object(<abbr>\<p\><\/p\></abbr>)
- Object image(<abbr>[IMG][\IMG]</abbr>)
- Object location with bounding boxes(<abbr>\<coor\>\</coor\></abbr>)

Only image regression loss for output visual embeddings is used.
During fine-tuning, visual encoder are frozen.

<details>
<summary>(toggle) Fine tuning detail</summary>
<div markdown="1">
<h4> Dataset </h4>
- High-quality datasets
    - Grounded text-to-image generation: CapsFusion-grounded-100M, GRIT
    - Image editing: InstructPix2Pix
    - Text-to-image task: filtered CapsFusion, LAION-Aesthetics, SA-1B, LAION-High_Resolution
- Some premium sources
    - Unsplash
- Output of Text-to-image systems
    - Midjourney-V5, DALL-E-3

<h4> Data argumentation </h4>
- Randomly drop entities and object localization imformation: for adaptability and robustness.
- Random background variations: reduce the reliance on image backgrounds.
- Random crop
</div>
</details>

<hr>

## Experiments
### In-context learning (base model)
With base model, zero-shot and few-shot abilities are evaluated on some VQA tasks.<br>

Result
![Image](/images/2024-10-30-review-emu2-07.png){: width="400"}
- Emu2 model shows **powerful in-context ability**
- Improved performance with more samples.
- Outperforms Flamingo-80B and IDEFICS-80B with smaller model (37B)

Here are some few-shot examples.

![Image](/images/2024-10-30-review-emu2-08.png)

### Academic-task-oriented benchmarks (Emu2-Chat)
Emu2-Chat is evaluated on academic-task-oriented benchmarks.<br>

Result
![Image](/images/2024-10-30-review-emu2-09.png)
- Outperforms other models in **visual question answering task**.
- Also better results on LMM benchmarks.

Here are some multimodal understanding examples.

![Image](/images/2024-10-30-review-emu2-10.png)

### Controllable visual generation (Emu2-Gen)
- Visual encoder and decoder of Emu2 show superior results on **autoencoding tasks**.
![Image](/images/2024-10-30-review-emu2-11.png){:width="400"}

- Emu2 achieves the state-of-the-art performance among multimodal model in **zero-shot text-to-image generation task**.
![Image](/images/2024-10-30-review-emu2-12.png){:width="400"}

- Emu2 shows strong **zero-shot subject-driven image editing** ability on DreamBench.
![Image](/images/2024-10-30-review-emu2-13.png){:width="400"}

Here are some examples of controllable generation.

![Image](/images/2024-10-30-review-emu2-14.png)
<hr>

## Summary
- Demonstrate strong in-context learning ability of LMM using new model, Emu2.
- By instruction tuning, Emu2-Chat and Emu2-Gen can follow specific task instructions.
- Emu2-Chat demonstrates SOTA results on some VQA and LMM benchmarks.
- Emu2-Gen shows remarkable capability of controllable visual generation.